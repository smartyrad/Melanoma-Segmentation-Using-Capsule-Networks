{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import cv2\n",
    "from os.path import join\n",
    "import os\n",
    "import random\n",
    "from functools import partial\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Reshape, concatenate\n",
    "from keras.optimizers import Nadam\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import backend as K\n",
    "from keras.backend import binary_crossentropy\n",
    "import tensorflow as tf\n",
    "\n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "print(K.image_data_format())\n",
    "smooth = 1e-12\n",
    "num_examples = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyingclahe(image):\n",
    "    gray = image\n",
    "    gray = cv2.cvtColor(gray, cv2.COLOR_BGR2GRAY)\n",
    "    ret, gray = cv2.threshold(gray, 250, 255,0)\n",
    "    image, contours, hierarchy = cv2.findContours(gray, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    largest_areas = sorted(contours, key=cv2.contourArea)\n",
    "    x,y,w,h = cv2.boundingRect(largest_areas[-1])\n",
    "    mask = np.zeros((512,512),dtype= np.uint8)\n",
    "    cv2.drawContours(mask, [largest_areas[-1]], 0, (255,255,255,255), -1)\n",
    "    image = cv2.bitwise_and(orig_images, orig_images, mask=mask)\n",
    "    roi=image[y-20:y+h+20,x-20:x+w+20]\n",
    "    roi=cv2.resize(roi,(512,512))\n",
    "    #plt.imshow(roi)\n",
    "    #plt.show()\n",
    "\n",
    "    lab= cv2.cvtColor(roi, cv2.COLOR_BGR2LAB)\n",
    "    #plt.imshow(lab)\n",
    "    #plt.show()\n",
    "    #-----Splitting the LAB image to different channels-------------------------\n",
    "    l, a, b = cv2.split(lab)\n",
    "    #plt.imshow(l)\n",
    "    #plt.show()\n",
    "\n",
    "    #plt.imshow(a)\n",
    "    #plt.show()\n",
    "\n",
    "    #plt.imshow(b)\n",
    "\n",
    "    #plt.show()\n",
    "    #-----Applying CLAHE to L-channel-------------------------------------------\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "    cl = clahe.apply(l)\n",
    "    #plt.imshow( cl)\n",
    "    #plt.show()\n",
    "    #-----Merge the CLAHE enhanced L-channel with the a and b channel-----------\n",
    "    limg = cv2.merge((cl,a,b))\n",
    "    #plt.imshow(limg)\n",
    "    #plt.show()\n",
    "    #-----Converting image from LAB Color model to RGB model--------------------\n",
    "    final = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "    plt.imshow(final)\n",
    "    plt.show()\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_images= cv2.imread(\"/home/ubuntu/research/train/btrain/4117.png\")#4117\n",
    "orig_images = cv2.resize(orig_images, (512, 512))\n",
    "rows,cols,_ = orig_images.shape\n",
    "\n",
    "# M = cv2.getRotationMatrix2D((cols/2,rows/2),180,1)\n",
    "# orig_images = cv2.warpAffine(orig_images,M,(cols,rows))\n",
    "#plt.imshow(orig_images)\n",
    "sdd = applyingclahe(orig_images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
